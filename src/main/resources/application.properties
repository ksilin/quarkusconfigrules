# default is 1, change that
kafka-streams.producer.acks=all

# we are allowing some kinds of compression, disallowing any others
kafka-streams.producer.compression.type=gzip

# default is 0. This can lead to long interruptions on rebalance
kafka-streams.num.standby.replicas=2

# if kafka streams, then application-id must be defined
kafka-streams.application-id=myApp_v001

# TODO - enable.idempotence if it is there at all, make sure its true


# TODO - check for overriding options with ENV vars
kafka.bootstrap.servers=${BOOTSTRAP_SERVERS:localhost:19092}

# TODO - check for SASL mechanism
kafka.sasl.mechanism=PLAIN

# check for security protocol
# kafka.security.protocol=SASL_PLAINTEXT
kafka.security.protocol=SASL_SSL

# TODO - check for presence & overriding with ENV
%test.kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
     username="USERNAME" \
     password="PASSWORD";

# TODO - check for topic check
quarkus.kafka-streams.topics=${kennzeichen.source-topic},${kennzeichen.target-topic}

# TODO - check for metrics being available
quarkus.kafka-streams.metrics.recording.level=DEBUG

# TODO - check for usable log configs
quarkus.log.category."org.apache.kafka.clients".level=DEBUG
%dev.quarkus.log.level=INFO
%test.quarkus.log.level=INFO

# check for max.poll.records being within a reasonable range
kafka-streams.consumer.max.poll.records=200
%dev.kafka-streams.consumer.max.poll.records=600



# error handling

# TODO - if not LogAndFail - verify WHY
# default.deserialization.exception.handler

# TODO - if overwritten - verify WHY
# default.production.exception.handler

# if processing.guarantee not default -> exactly_once_v2


